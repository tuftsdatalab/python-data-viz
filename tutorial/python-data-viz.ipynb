{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a94363f6-56ac-48e2-a77b-a01f473ce956",
   "metadata": {},
   "source": [
    "# Python for Data Analysis and Visualization\n",
    "\n",
    "---\n",
    "\n",
    "**A Tufts University Data Lab Tutorial**  \n",
    "Written by [Uku-Kaspar Uustalu](https://directory.tufts.edu/user/view/90E8E773F8EC92B23679584546E5E321/)\n",
    "\n",
    "Contact: <uku-kaspar.uustalu@tufts.edu>\n",
    "\n",
    "Last updated: `GH_ACTIONS_DATE`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d330a0d1-d7db-4d87-b8a7-f7fe3985332b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Importing Packages\n",
    "\n",
    "We will be using the following Python data analysis and visualization libraries throughout this tutorial:\n",
    "\n",
    "- [Pandas](https://pandas.pydata.org/pandas-docs/stable/index.html) is the primary data analysis library in Python. It allows for easy analysis and manipulation of tabular data and is usually imported under the alias `pd`.\n",
    "- [Matplotlib](https://matplotlib.org/) is the most essential data visualization library in Python. Although it consists of many modules, most of the plotting functionality is contained within the `matplotlib.pyplot` module, which is usually imported under the alias `plt`.\n",
    "- [Seaborn](https://seaborn.pydata.org/) is an advanced plotting library that is built on top of Matplotlib. It has a simpler interface and allows for the easy creation of beautiful visualizations. Seaborn is usually imported under the alias `sns`.\n",
    "- [HVPlot](https://hvplot.holoviz.org/) is a high-level plotting interface that integrates seamlessly with Pandas and allows for the easy creation of interactive visualizations. The `hvplot.pandas` module must be imported to allow for seamless integration with Pandas.\n",
    "- [Plotly](https://plotly.com/python/) is an alternative interactive visualization library. It consists of many modules, but the `plotly.express` module is the easiest to use as it allows for the creation of whole plots using a single command. The module is usually imported under the alias `px`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7199e6cd-77ea-4d60-808e-8831582b6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hvplot.pandas\n",
    "import plotly.express as px"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9aea8562-9b46-46a1-97ef-51bfadf2f2bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Getting Started with Pandas\n",
    "\n",
    "For the first part of this tutorial, we will be using the following datasets from the `data` directory to investigate the relationship between health and wealth:\n",
    "\n",
    "- [`gdp.csv`](./data/gdp.csv) -- World Bank gross domestic product (GDP) estimates (in USD) for world countries and regions from 1960 until 2021\n",
    "- [`life-expectancy.csv`](./data/life-expectancy.csv) -- World Bank life expectancy estimates for world countries and regions from 1960 until 2020\n",
    "- [`m49.csv`](./data/m49.csv) -- United Nations [M49](https://en.wikipedia.org/wiki/UN_M49) Standard Country or Area Codes for Statistical Use\n",
    "- [`population.csv`](./data/population.csv) -- World Bank population estimates for world countries and regions from 1960 until 2021\n",
    "\n",
    "All the datasets are in [IEFT RFC 4180 CSV](https://www.rfc-editor.org/info/rfc4180) (comma-separated values) format and the first four rows of the World Bank data files contain metadata with the actual data table starting on row five.\n",
    "\n",
    "Let us start by reading in the population data. Pandas can easily read CSV datasets via the [`pandas.read_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function. The function reads the contents of the file into a [`pandas.DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) data structure and supports various additional arguments. For example, we can utilize the `skiprows` argument to tell Pandas to skip the first four rows of the dataset as the data table does not start until fow five.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744b19c-db10-4f4f-ba7e-ad84069bfae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "population = pd.read_csv(\"data/population.csv\", skiprows=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef3e90-bb5f-4904-8320-355c2b0d3a99",
   "metadata": {},
   "source": [
    "Now the World Bank population dataset is stored in a DataFrame called `population`. Calling the DataFrame by its name will display the first and last five rows of the table by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb99f7c-458a-4d9e-9d6e-309d6865bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "population"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2fc78cd-36a1-495f-95a3-5b84f73430a5",
   "metadata": {},
   "source": [
    "We see that the DataFrame appears to have the following columns:\n",
    "\n",
    "- `Country Name` -- English name of the country\n",
    "- `Country Code` -- [ISO 3166-1 alpha-3](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3) country code\n",
    "- `Indicator Name` -- name of the indicator represented by the data\n",
    "- `Indicator Code` -- World Bank code for the indicator\n",
    "- `1960` ... `2021` -- population estimates by year\n",
    "\n",
    "We also see that the DataFrame has 266 rows and 66 columns. We can double-check this by looking at the value of the [`pandas.DataFrame.shape`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html) attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ed0a2-b870-42da-8342-1dcef2b2af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "population.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1278d0b-da95-4c83-b022-55926efba5b0",
   "metadata": {},
   "source": [
    "The [`pandas.DataFrame.size`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.size.html) attribute will give us the total number of values in the table (number of columns times number of rows).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400dae1-baad-4298-bbad-91d08a985fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "population.size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb2fd4d6-ab6c-4c78-8c15-2e9282d7c96e",
   "metadata": {},
   "source": [
    "[`pandas.DataFrame.columns`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.columns.html) can be used to get a list of all the column names and [`pandas.DataFrame.dtypes`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html) will display the datatype of each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044c2bd-41aa-4a58-9788-d246df3ed18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "population.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d630ee-18fb-4e63-b6d9-0e87ad547e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "population.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bd83f00-9526-4ecb-9780-f52e3560ec8f",
   "metadata": {},
   "source": [
    "Note how the first four columns all have the `object` datatype. This could mean that the column contains textual data (string), has a mix of different datatypes (both textual and numeric for example), or contains a more complex data structure (like a list or tuple). The population columns are all `float64` denoting floating-point numbers. It might feel odd to store population values as floating-point numbers as population counts are always whole integers. However, in Pandas all numeric data is stored as floating-point numbers by default. This is due to the fact that integer columns in Pandas do not support missing data values by default. The default missing data value in Pandas is the `numpy.nan` from NumPy, which is a `float64` datatype.\n",
    "\n",
    "We know that the `population` DataFrame stores population values, so the `Indicator Name` and `Indicator Code` columns are redundant. We can drop them from the table using the [`pandas.DataFrame.drop()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f8f00-a30c-46a0-b91a-47cc9437776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "population.drop(columns=[\"Indicator Name\", \"Indicator Code\"], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8051da39-fd74-4ea4-b540-627e48284458",
   "metadata": {},
   "source": [
    "Note how we specified two arguments when calling the [`pandas.DataFrame.drop()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) method. First we specified a list of columns to drop using the `columns` argument. The [`pandas.DataFrame.drop()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) method also supports dropping rows, so that is why the `columns` argument is needed. Then we also specified `inplace` to be `True`. This ensures that the original `population` DataFrame gets modified. Otherwise the method would just return a new DataFrame and keep the `population` DataFrame unchanged.\n",
    "\n",
    "We can validate that the desired columns have been removed by taking a quick peek at the DataFrame via the [`pandas.DataFrame.head()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) method. It displays the fist five rows of the DataFrame by default but you can also pass the number of rows desired as an argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d1ba5-d0ae-4dfc-8d4b-948c3db2d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "population.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23fbb3ad-729e-4f52-af57-d75af9ee583c",
   "metadata": {},
   "source": [
    "Knowing that the World Bank GDP dataset follows the exact same format as the World Bank population dataset, we can read it in and drop the `Indicator Name` and `Indicator Code` columns all in one go by chaining together the [`pandas.read_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function and the [`pandas.DataFrame.drop()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) method. If we want to include a line break somewhere in the chain, we need to wrap the whole thing in parentheses `()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651e2e0-25d0-4411-bc58-09cb7f7ec667",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp = pd.read_csv(\"data/gdp.csv\", skiprows=4).drop(\n",
    "    columns=[\"Indicator Name\", \"Indicator Code\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c65f229-415d-47d2-84c9-c7d66fb66d33",
   "metadata": {},
   "source": [
    "Note how here we did not specify `inplace=True` when dropping the columns. That is because we want the [`pandas.DataFrame.drop()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) method to take the DataFrame generated by [`pandas.read_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) and then output a new DataFrame that we can save into the `gdp` variable. We can take a look at our newly created DataFrame by using the [`pandas.DataFrame.head()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) method again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c039a4-ad72-491f-95f0-6387b0283c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22f4d5a0-6900-4523-acf6-bb518347aa6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Long vs Wide Data\n",
    "\n",
    "GDP on its own is not a good indicator of the wealth of a country as countries with more people tend to have higher GDP. But if we were to normalize GDP by population, then the resulting GDP per capita values can be compared across countries and used as a proxy for wealth. To do so, we must be able to match up the GDP and population values for each unique combination of country and year.\n",
    "\n",
    "The GDP and population tables currently are in wide format -- each row represents a unique country and each column represents a unique year with the cell values representing unique population estimates. While this wide format has many advantages and is commonly used in geospatial applications, it does complicate joining various datasets. One option would be to treat both tables as matrices and calculate GDP per capita by dividing the GDP matrix with the population matrix. However, both tables need to have the exact same layout with the same number of countries and years in the same exact order for this to work and the result to be reliable. Ensuring this is not a trivial task, so this method would involve a lot of work to produce reliable results.\n",
    "\n",
    "Alternatively the two tables could be joined by country. Then we will have an extra-wide table with two sets of year columns -- one set of year columns for population and another set of year columns for GDP. Then we would need to create another new column for each year by dividing the corresponding GDP column with the corresponding population column, resulting in another new set of year columns. As you can see, this approach would quickly lead to a very messy and difficult to manage dataset and would also involve a lot of work, making it far from preferred.\n",
    "\n",
    "The easiest option for calculating GDP per capita would involve converting both datasets into a long format, where each row represents a single unique observation (estimation). Instead of having countries in rows and years in columns, each row would instead represent a unique country and year combination. This would allow us to easily combine datasets on both country and year, ensuring that the GDP and population values for each country-year combination get matched.\n",
    "\n",
    "We can use the [`pandas.DataFrame.melt()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.melt.html) method to convert wide DataFames to long format. We need to specify three arguments when using this method:\n",
    "\n",
    "- `id_vars` -- name(s) of the column(s) that define a unique observation in the original wide dataset\n",
    "- `var_name` -- name of the the column in the new long dataset that stores the column names of the original wide dataset\n",
    "- `value_name` -- name of the column in the new long dataset that stores the values of the original wide dataset\n",
    "\n",
    "Each observation in the original wide dataset represents a unique country defined either by the country name or country code. Let us include both of these as `id_vars` to carry both columns over to the long dataset. The columns of the wide dataset represent years, so that is the name we will pass on to the `var_name` argument. The values of the wide dataset represent population estimates, so that will be the name passed on to the `value_name` argument.\n",
    "\n",
    "The reverse command for [`pandas.DataFrame.melt()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.melt.html) is [`pandas.DataFrame.pivot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html), which can convert a long format table to wide format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef976e-1d4b-4196-a939-29669bcd032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_long = population.melt(\n",
    "    id_vars=[\"Country Name\", \"Country Code\"], var_name=\"year\", value_name=\"population\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a15cd0-209e-4b8d-aa0b-31c82873409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c6b7f4-7a30-4103-a6b6-ea4803dae590",
   "metadata": {},
   "source": [
    "Now we have a new long population DataFrame called `population_long`, where each row represents a unique country and year combination. Let us use `pandas.DataFrame.dtypes` to confirm the data types of this new table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01023410-99c9-4175-bbce-e9b4a11edfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_long.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f856994-6b7f-4de9-ba6f-60755b968b4a",
   "metadata": {},
   "source": [
    "Note how the `year` column is of type `object`, meaning that the years are currently stored as strings. As the years were perviously column names, this makes sense. However, as years are actually numbers, they should also be stored as such to allow for easy comparisons and mathematical operations.\n",
    "\n",
    "To convert the year values to integers, we must first extract the `year` column as a [`pandas.Series`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) object. This can be done by either using square brackets `df[\"column\"]` or via dot-notation `df.column`. The latter requires the column name to consist of only letters, numbers, and underscores (and not start with a number), so it is only useful if the column names are neatly formatted. Using square brackets to extract columns is more robust and as the column name is passed as a string, it can contain spaces and other special characters.\n",
    "\n",
    "Square brackets can be used to also create a new column or overwrite an existing column. Dot-notation should **only** be used to read columns. Attempting to write columns using dot-notation could have unexpected consequences.\n",
    "\n",
    "Knowing this, let us extract the `year` column as a Series object using dot-notation `df.column` and then call [`pandas.Series.astype()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.astype.html) on the extracted values to convert them to integers. Then we can use square bracket notation `df[\"column\"]` to replace the values of the `year` column with their integer equivalents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527d81e-8f90-41e0-b6f8-bbc78a253dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_long[\"year\"] = population_long.year.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa230d-36de-46f3-abf5-b6bb47a04c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e985dd4-4562-44a1-9f71-8ac37497b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_long.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f211b-c738-44d2-b5ab-9e2ff7d7dba5",
   "metadata": {},
   "source": [
    "Note how the values of the `year` column seemingly did not change, but the datatype of the values is now `int32`, which means that the values have been converted to numeric integers.\n",
    "\n",
    "Now let us convert the GDP dataset to long format as well. We can chain the [`pandas.DataFrame.melt()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.melt.html) method together with the [`pandas.DataFrame.astype()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html) method to convert the DataFrame from wide to long format and change the datatype of the `year` column to integer all in one go. The [`pandas.DataFrame.astype()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html) method is very similar to the [`pandas.Series.astype()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.astype.html) method, but instead of taking a single datatype as an argument, it takes a dictionary that maps column names to datatypes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e487f-1644-4748-a40c-b6f42cf75638",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_long = gdp.melt(\n",
    "    id_vars=[\"Country Name\", \"Country Code\"], var_name=\"year\", value_name=\"gdp\"\n",
    ").astype({\"year\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b346e5-d428-4205-81bb-a41edaa691da",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f779178-e5c3-440c-9a78-8f05979251eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_long.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7eb55ecd-a0b0-4883-a0a3-6ad43db99567",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Joining Datasets\n",
    "\n",
    "Finally we are ready to combine the population and GDP datasets. [`pandas.DataFrame.merge()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) can be used to perform a join on on one or more columns. The method is called on the left DataFrame and takes the right DataFrame as its first argument (this is only important to know when performing a left or right join). Additional arguments are as follows:\n",
    "\n",
    "- `on` -- A single column name (string) or list of column names to join on. These column names should appear in both tables. If the column names differ between datasets, the separate `left_on` and `right_on` arguments should be used instead.\n",
    "- `how` -- The type of join to perform. Here are the possible values:\n",
    "  - `\"left\"` -- use only keys from the left DataFrame (include all rows from left DataFrame)\n",
    "  - `\"right\"` -- use only keys from the right DataFrame (include all rows from right DataFrame)\n",
    "  - `\"outer\"` -- use the union of keys from both DataFrames (include all rows from both DataFrames)\n",
    "  - `\"inner\"` -- use the intersection of keys from both DataFrames (include only matching rows)\n",
    "  - `\"cross\"` -- creates the cartesian product from both DataFrames (similar to cross-tabulation)\n",
    "\n",
    "We would like to join on each unique country and year combination. As spellings of country names might differ between datasets, it is good practice to always use the [ISO 3166-1 alpha-3](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3) country code or some other analogous unique identifier to distinguish between countries. The country code for each country is determined by an international standard and should not differ between datasets, allowing us to reliably join the data. Hence we will specify `on=[\"Country Code, \"year\"]` to perform the join on unique country-year combinations and `how=\"inner\"` to only keep year-country combinations that are present in both datasets. Since we do not want the `Country Name` column repeated in the joined dataset, we should remove it from the GDP table using [`pandas.DataFrame.drop()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) before performing the join. Otherwise the `Country Name` column from the GDP dataset will also get joined, resulting in the joined table having two separate columns with country names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b1851e-3f21-4e85-a128-cf73f7fd78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = population_long.merge(\n",
    "    gdp_long.drop(columns=\"Country Name\"), on=[\"Country Code\", \"year\"], how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af607d13-d44c-4a72-9909-3ef9f26762c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838ad2d3-fcc8-4ea6-b21a-34e76d054edb",
   "metadata": {},
   "source": [
    "Now we have a table with a population and GDP value for each country and year combination. We can easily add a new column denoting GDP per capita to this table by dividing the GDP column with the population column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb782928-c205-4756-b3da-8efc1eef11ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"gdp_per_capita\"] = data.gdp / data.population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d6ff2c-5274-4948-afa7-d8ba036ee3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239c1d48-2e25-4dc5-9fa4-dbaf95273f17",
   "metadata": {},
   "source": [
    "Now we would also like to add life expectancy information to this joined dataset. Knowing that all World Bank data tables follow the same format, we can easily convert the workflow from before into a function that reads in a World Bank dataset, drops unneeded columns, converts it to long format, and ensures the year is in numeric format. That function would only need two inputs -- the path of the CSV file and the name of the indicator represented by the data. (This name will be used as the colum name for the values column in the long format table.) Let us define this function and use it to read in the World Bank life expectancy dataset and convert it to long format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f007d-e38d-4577-a9ea-b630abe30d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_world_bank_data(file_name, value_name):\n",
    "    return (\n",
    "        pd.read_csv(file_name, skiprows=4)\n",
    "        .drop(columns=[\"Indicator Name\", \"Indicator Code\"])\n",
    "        .melt(\n",
    "            id_vars=[\"Country Name\", \"Country Code\"],\n",
    "            var_name=\"year\",\n",
    "            value_name=value_name,\n",
    "        )\n",
    "        .astype({\"year\": int})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a8150-6632-4ea9-8102-664c8da30533",
   "metadata": {},
   "outputs": [],
   "source": [
    "life_exp = read_world_bank_data(\n",
    "    file_name=\"data/life-expectancy.csv\", value_name=\"life_exp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90450b9e-194c-4692-af9b-6c59eaa0982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "life_exp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c45a8fc-4824-422d-88d8-0e2c23a1d966",
   "metadata": {},
   "source": [
    "Using the same workflow from before, we can join the long format life expectancy dataset to our table containing the GDP and population data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd2207d-a7e2-4396-b7de-bb60898e9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(\n",
    "    life_exp.drop(columns=\"Country Name\"), on=[\"Country Code\", \"year\"], how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb161df-eb0b-4004-beed-be5c56fd2504",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7ba039-7bd4-4db4-82b1-2d229befe981",
   "metadata": {},
   "source": [
    "Finally we would also like to know which [United Nations regional geoscheme](https://en.wikipedia.org/wiki/United_Nations_geoscheme) the country belongs to. Information on this is available in the United Nations [M49](https://en.wikipedia.org/wiki/UN_M49) dataset. As this dataset is a standard CSV table, we can use [`pandas.read_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) without any additional arguments to read it into a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb26b7e-00f4-45eb-9ffe-a9bf38712618",
   "metadata": {},
   "outputs": [],
   "source": [
    "m49 = pd.read_csv(\"data/m49.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9406a831-faff-47a4-9a13-b697480f8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "m49.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff49e5e-2e16-4d04-aa03-8930daa910e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m49.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faac402-25aa-4d27-b052-08252eeedd8e",
   "metadata": {},
   "source": [
    "Note how this dataset contains a lot of information on the various groups and codes assigned to each country. We are only interested in the name of the region the country belongs into and the [ISO 3166-1 alpha-3](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3) code assigned to the country. Using double square brackets `[[ ]]` we can extract the desired columns as a new DataFrame. (In reality we are just passing a list of column names to the standard single square brackets indexer.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3912b5c6-2b40-42e1-b884-c650a100a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = m49[[\"Region Name\", \"ISO-alpha3 Code\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a52444f-2ea5-4326-8e4e-52c06fd92883",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7fdbe7-202d-44f8-aed0-d5bfac8bbdaf",
   "metadata": {},
   "source": [
    "Now we can use [`pandas.DataFrame.merge()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) again to join the regions to the rest of our data. Since the names of the columns containing the country code information differ between the datasets, we must use the `left_on` and `right_on` arguments instead of the `on` argument from before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e61add-145d-4828-bb31-8a449f638014",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(\n",
    "    regions, left_on=\"Country Code\", right_on=\"ISO-alpha3 Code\", how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da80a646-84b5-4ccb-bfb7-60a8a80e9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46487b19-4713-46e9-8445-19e791d7af67",
   "metadata": {},
   "source": [
    "Note how the new joined dataset contains both of the country code columns (because their names were different). Also, the naming convention in our table is not uniform -- some column names are in [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) (which is preferred) while others contain spaces and a mix of uppercase and lowercase letters. Let us use [`pandas.DataFrame.drop()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) to drop the second country code column and [`pandas.DataFrame.rename()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) to rename some of the columns to ensure an uniform column naming convention. Remember that we can use the `inplace=True` argument to apply the changes to the original DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b977f2f9-591b-4bff-8687-ff7fb8b24266",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=\"ISO-alpha3 Code\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6832f-063c-4d58-b1b5-c694a7df7f0c",
   "metadata": {},
   "source": [
    "[`pandas.DataFrame.rename()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) takes a dictionary in the format `{\"old_name\": \"new_name\"}` as an argument and you need to specify whether you would like to rename rows or columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203d5f9-8b2d-41b7-8872-8962ce685e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(\n",
    "    columns={\n",
    "        \"Country Name\": \"country_name\",\n",
    "        \"Country Code\": \"country_code\",\n",
    "        \"Region Name\": \"region_name\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c96b445-0c28-40f3-963b-877504157146",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a8d8a2-8bb5-4c9c-8fc2-920cf9993307",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Boolean Indexing\n",
    "\n",
    "To extract specific rows from a DataFrame, we can combine the square brackets indexing operator `pandas.DataFrame[]` with a logical operation that produces a boolean array. This would select every row from the DataFrame where the corresponding element in the boolean array equals `True`. For example, to extract all rows that correspond to the United States, we could use `data.country_code == \"USA\"`. This would return an array of `True` and `False` values where the value of a specific element in the array is `True` if the corresponding row in the `data` DataFrame had the value `\"USA\"` in its `country_code` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb83974b-9da0-47d7-b200-4f9a97a358c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.country_code == \"USA\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba1d21-9875-4333-b79c-14f639386b91",
   "metadata": {},
   "source": [
    "Combining this with the square brackets indexing operator `data[]` will extract all values from the `data` DataFrame where the `country_code` column has the value `\"USA\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5829e274-d5b8-4af2-9d86-7f1b789eec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_data = data[data.country_code == \"USA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c72ad-4d05-44a3-bf8d-ceb17f64ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5601cda-ff1a-4a5e-88df-52b8cfa2a2a7",
   "metadata": {},
   "source": [
    "We can ensure that this new `usa_data` DataFrame only contains values corresponding to the United States by calling [`pandas.Series.unique()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.unique.html) on the `country_name` column. This will return an array of all the unique country names present in the table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e41588-11a3-4b31-99a0-44b9f2fbdd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_data.country_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38319a58-57de-4901-9506-29eecf4e36e4",
   "metadata": {},
   "source": [
    "Note that even though there is only one unique value, the result is still an array. To extract the value as a string, we must extract the first element of the array using `[0]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea72f0e-d443-4002-8b19-e3f76889a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_data.country_name.unique()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab182b00-2284-4a59-aceb-ced28829f8db",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating Static Line Graphs\n",
    "\n",
    "[Matplotlib](https://matplotlib.org/) is the primary plotting library in Python and it is designed to resemble the plotting functionalities of MATLAB. While it provides all kinds of different plotting functionality, the [`matplotlib.plyplot`](https://matplotlib.org/stable/api/pyplot_summary.html) module is used the most. It is common to import this module under the alias `plt` as we did before. Matplotlib works in a layered fashion. First you define your plot using [`matplotlib.pyplot.plot(x, y, ...)`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html), then you can use additional [`matplotlib.plyplot`](https://matplotlib.org/stable/api/pyplot_summary.html) methods to add more layers to your plot or modify its appearance. Finally, you use [`matplotlib.pyplot.show()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html) to display the plot or [`matplotlib.pyplot.savefig()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html) to save it to an external file.\n",
    "\n",
    "The `x` and `y` arguments in the [`matplotlib.pyplot.plot()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html) call can be either arrays or [`pandas.Series`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) objects. For example, we can visualize the population of the United States over time by extracting the `year` and `population` columns of the `usa_data` table as [`pandas.Series`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) objects and passing them along to [`matplotlib.pyplot.plot()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html) as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb4e4d-e062-4e1b-af12-62f69ddbb97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(usa_data.year, usa_data.population)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c09ca5-d0cc-422e-afce-31666748d781",
   "metadata": {},
   "source": [
    "Alternatively we could pass the [`pandas.DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) to the [`matplotlib.pyplot.plot()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html) command using the optional `data` argument. This will allow us to specify the desired column names as the `x` and `y` arguments instead of having to extract them as [`pandas.Series`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) objects. For example, we can visualize the GDP of the United States over time as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f63f3-970d-4b31-9bae-80fed17b9cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\"year\", \"gdp\", data=usa_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5138981-9234-47b6-a1e3-44aba7736b76",
   "metadata": {},
   "source": [
    "[Pandas](https://pandas.pydata.org/pandas-docs/stable/index.html) also has built-in plotting functionality via the [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) method. It takes the column names of the `x` and `y` columns as arguments and uses a plotting backend to generate the plot. By default, the plotting backend is Matplotlib, but this could be reconfigured to be something else instead. For example, we can create a Matplotlib visualization showing United States life expectancy over time as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd45aeb-c551-4ae9-8685-e8dcf8f63e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_data.plot(x=\"year\", y=\"life_exp\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5879be18-b111-4772-b7cd-4c3981b4b30e",
   "metadata": {},
   "source": [
    "To create a line graph with multiple lines, we need to stack the lines using multiple [`matplotlib.pyplot.plot()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html) or [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) calls. But how can we specify that we would like to stack the lines onto a single plot instead of creating a new plot for each line? This is where the [`matplotlib.axes.Axes`](https://matplotlib.org/stable/api/axes_api.html#the-axes-class) class comes into play. For simplicity, you can think of each [`matplotlib.axes.Axes`](https://matplotlib.org/stable/api/axes_api.html#the-axes-class) object as a canvas onto which one can add multiple layers of visualization. When using [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) to create visualizations, we can utilize [`matplotlib.axes.Axes`](https://matplotlib.org/stable/api/axes_api.html#the-axes-class) to create multi-layered plots as follows:\n",
    "\n",
    "1. The first [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) command will return a [`matplotlib.axes.Axes`](https://matplotlib.org/stable/api/axes_api.html#the-axes-class) object. This object should be saved into a variable. It is common to save it into a variable called `ax`.\n",
    "2. In each subsequent [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) call, the [`matplotlib.axes.Axes`](https://matplotlib.org/stable/api/axes_api.html#the-axes-class) object from before should be passed on using the `ax` argument. This will ensure the new plot gets added to the same canvas.\n",
    "\n",
    "We can combine the [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) call with boolean indexing to easily visualize subsets of the data and use the `color` and `label` arguments to specify a color and legend label for each subset.\n",
    "\n",
    "Once all the lines have been added to the plot, we can use [`matplotlib.pyplot.ylabel()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.ylabel.html) and [`matplotlib.pyplot.xlabel()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xlabel.html) to label the axes and [`matplotlib.pyplot.title()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.title.html) to specify a title for the visualization. Finally we call [`matplotlib.pyplot.show()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html) to display the plot.\n",
    "\n",
    "Knowing all this, a visualization illustrating the GDP per capita over time for North American countries can be generated as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c465db-3fdf-4b94-9cd4-553ab2c9b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = data[data.country_code == \"USA\"].plot(\n",
    "    x=\"year\", y=\"gdp_per_capita\", color=\"blue\", label=\"USA\"\n",
    ")\n",
    "\n",
    "data[data.country_code == \"CAN\"].plot(\n",
    "    x=\"year\", y=\"gdp_per_capita\", color=\"red\", label=\"Canada\", ax=ax\n",
    ")\n",
    "\n",
    "data[data.country_code == \"MEX\"].plot(\n",
    "    x=\"year\", y=\"gdp_per_capita\", color=\"green\", label=\"Mexico\", ax=ax\n",
    ")\n",
    "\n",
    "plt.ylabel(\"GDP per capita\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.title(\"GDP per Capita Over Time for North American Countries\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa2c70f4-973e-45b2-855c-ba9a420d34bc",
   "metadata": {},
   "source": [
    "There are many benefits to using [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) over [`matplotlib.pyplot.plot()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html) when dealing with DataFrames. Most importantly, [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) interacts directly with a Pandas DataFrame and has a much simpler user interface with numerous named arguments allowing for easy customization. However, when it comes to more advanced tasks, Matplotlib allows for better fine-tuning and more flexibility. However, this comes at a cost of more complex commands. For example, to create a plot that displays the temporal variation of both the life expectancy and GDP per capita of the United States using two different Y axes, we must use a relatively advanced workflow.\n",
    "\n",
    "First, we define the size of our plot using the `figsize` argument of [`matplotlib.pyplot.subplots()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html). This command allows for the creation of multiple subplots, but is also frequently used to specify the size of a single plot. It returns a tuple consisting of a [`matplotlib.figure.Figure`](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure) and a [`matplotlib.axes.Axes`](https://matplotlib.org/stable/api/axes_api.html#the-axes-class) object.\n",
    "\n",
    "To add a plot layer to a specific [`matplotlib.axes.Axes`](https://matplotlib.org/stable/api/axes_api.html#the-axes-class) object, we can use [`matplotlib.axes.Axes.plot()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.plot.html) which works very similarly to the previously discussed [`matplotlib.pyplot.plot()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html) command. Both commands take an optional format string as the third positional argument that allows you to specify the line and marker style and color using a simple shorthand. For example, `\"g--\"` means a green dashed line and `\"mx\"` indicates magenta-colored X-shaped markers. Refer to the function documentation for a full overview of all the shorthand characters. The Matplotlib commands for adding axes labels and plot titles also have additional arguments that modify the appearance of the label or title. For example, `color` usually specifies the text color and `size` is used to specify the size of the font.\n",
    "\n",
    "To add another Y axis to the plot, we can use [`matplotlib.axes.Axes.twinx()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.twinx.html) to create another [`matplotlib.axes.Axes`](https://matplotlib.org/stable/api/axes_api.html#the-axes-class) object that defines a new Y axis but shares the same X axis.\n",
    "\n",
    "Finally, we can use [`matplotlib.figure.Figure.legend()`](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.legend) to add a legend to the whole figure (including all the axes objects).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ce090-ea0a-46ec-a501-2e1eb6be20f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.plot(usa_data.year, usa_data.gdp_per_capita, \"g--\", label=\"GDP per Capita\")\n",
    "plt.ylabel(\"GDP per Capita\", color=\"g\")\n",
    "plt.xlabel(\"Year\")\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(usa_data.year, usa_data.life_exp, \"mx\", label=\"Life Expectancy\")\n",
    "plt.ylabel(\"Life Expectancy\", color=\"m\")\n",
    "plt.title(\"United States\", size=20)\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e8b7d-857a-41c9-b63b-d902f580a660",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualizing Distributions and Correlations\n",
    "\n",
    "Let us return to our original goal of exploring the relationship between health and wealth. We will use GDP per capita as a proxy for wealth and life expectancy as an indicator of health. We can simplify the analysis by looking only at one point in time and focus our analysis on 2020, which is the latest year we have both GDP per capita and life expectancy data available. We shall use boolean indexing to extract 2020 data into a new DataFrame called `data2020`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee29a7-98bc-4a80-877e-d0b79a1c50b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2020 = data[data.year == 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a818751-0b83-420b-af6d-6619a1d55f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2020.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d94f1-df8b-4e4d-9740-b9f96323a93f",
   "metadata": {},
   "source": [
    "How is wealth distributed amongst the global population? Let us get a vague idea by visualizing the distribution of GDP per capita amongst world countries in 2020. We can easily create an histogram by using the [`matplotlib.pyplot.hist()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html) command and passing it the GDP per capita [`pandas.Series`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c4f1b5-ad97-4c37-b10b-e489aa0ebccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data2020.gdp_per_capita)\n",
    "plt.xlabel(\"GDP per Capita\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532515d1-c811-4424-88dd-a378d8ad9a72",
   "metadata": {},
   "source": [
    "Note how we were able to easily create a histogram, but the result was quite ugly. If we wanted a prettier plot, we could go though the trouble of customizing the plot using various additional arguments and commands, which would take quite a while. Or we could use [Seaborn](https://seaborn.pydata.org/) which allows us to easily create beautiful visualizations with sensible defaults. For example, we could create a well-designed histogram with a smoothed kernel density estimate (KDE) overlay using the [`seaborn.histplot()`](https://seaborn.pydata.org/generated/seaborn.histplot.html) function along with the `kde=True` flag. Knowing this, let us look at the distribution of life expectancy amongst world countries in 2020.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84f15bd-0f29-4881-bad2-462383fe05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data2020.life_exp, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93efd341-f157-47b4-bcba-16ffffffd1af",
   "metadata": {},
   "source": [
    "To easily create a scatter plot analyzing the relationship between GDP per capita and life expectancy, we can use [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) and specify `kind=\"scatter\"` to ensure the result is a scatter plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bff821-140e-4b3f-8a6e-808f9e58ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2020.plot(x=\"gdp_per_capita\", y=\"life_exp\", kind=\"scatter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51cf70f-7fb9-4a76-91e6-289120d3d320",
   "metadata": {},
   "source": [
    "The relationship appears to be logarithmic. This is likely due to the distribution of GDP per capita being heavily skewed. We can easily confirm this by plotting a two-dimensional kernel density estimate (KDE) plot using [`seaborn.jointplot()`](https://seaborn.pydata.org/generated/seaborn.jointplot.html) along with `kind=\"kde\"`. (To get a scatter plot with histograms, one would use `kind=\"scatter\"`.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32205a2c-d451-416e-9652-a07b65f0cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=data2020, x=\"gdp_per_capita\", y=\"life_exp\", kind=\"kde\", fill=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09348-08d0-4fb0-babe-05996dbb4169",
   "metadata": {},
   "source": [
    "To get a better sense of the potentially logarithmic relationship between GDP per capita and life expectancy, we should apply a logarithmic transformation to the axis corresponding to GDP per capita. In our example this is the X axis and we can apply a logarithmic transformation on the X axis by passing `\"log\"` to the [`matplotlib.pyplot.xscale()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xscale.html) function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82fa13-adcf-40a6-a37e-78997c3a2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data2020.gdp_per_capita, data2020.life_exp)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"GDP per Capita\")\n",
    "plt.ylabel(\"Life Expectancy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd717f54-a731-4deb-9d6c-85cd84c397d9",
   "metadata": {},
   "source": [
    "Note how we used [`matplotlib.pyplot.scatter()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html) instead of [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) to create the scatter plot. Both functions are very similar and in reality, the latter simply calls the former. Also note how now the X axis of the scatter plot is logarithmic. This makes the relationship much clearer and we can quite definitely state that there appears to be a logarithmic relationship between life expectancy and GDP per capita.\n",
    "\n",
    "But does the size of a country play a role in this relationship? To find out, we can scale the size of the data points proportionally to the population such that bigger points indicate countries with more population. This can be done using the `s` argument in [`matplotlib.pyplot.scatter()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html), which takes an array of point sizes. This array needs to be the same size as the `x` and `y` arrays with one size value for each `x` and `y` combination. We can easily generate an array like this using the formula $X \\div max(X) \\times s$ where $X$ is the array we want to base the sizes on and $s$ is a scaling factor in arbitrary plot units. Note that the scaling factor is completely arbitrary and you might need to try different values until you find something that makes the visualization look good. We divide the input array with its maximum value to properly normalize and scale the sizes.\n",
    "\n",
    "Scaling the point sizes by population might cause some bigger points to overlap smaller ones. To ensure we can properly see overlapping points, we can use the `alpha` argument in the [`matplotlib.pyplot.scatter()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html) call to specify a transparency factor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a3e64-664d-48ad-9750-a55f715a222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(\n",
    "    data2020.gdp_per_capita,\n",
    "    data2020.life_exp,\n",
    "    s=data2020.population / data2020.population.max() * 5000,\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"GDP per Capita\")\n",
    "plt.ylabel(\"Life Expectancy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65afc9cd-0a48-4dee-9d23-23d9794c26e5",
   "metadata": {},
   "source": [
    "Looks like the size of a country does not seem to be related to GDP per capita or life expectancy. But what about the region a country is in? There is a good chance a correlation exists between the geographical location of a country and other indicators. To find out, we should color the points based on their geographic region. We know from before that this requires adding multiple layers to the plot -- one for each region. We can get a list of all the regions by using [`pandas.Series.unique()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.unique.html) on the `region_name` column. Then we can iterate over that list using a loop, subset the data for each region, and create a scatter plot layer using the subset data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88b77a-e948-40f5-82f9-53bae07aac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "for region in data2020.region_name.unique():\n",
    "\n",
    "    subset = data2020[data2020.region_name == region]\n",
    "\n",
    "    ax.scatter(\n",
    "        subset.gdp_per_capita,\n",
    "        subset.life_exp,\n",
    "        s=subset.population / data2020.population.max() * 5000,\n",
    "        label=region,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"GDP per Capita\")\n",
    "plt.ylabel(\"Life Expectancy\")\n",
    "plt.title(\"2020\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3f796d-7ea1-443d-b624-ef74bb97f77a",
   "metadata": {},
   "source": [
    "One of the main drawbacks on Matplotlib is the fact that one needs to create multiple layers to visualize groups using different colors. This can be a tedious process and usually involves having to subset the data using a loop. To circumnavigate this, many choose to use Seaborn instead, which allows for a grouping variable to be passed via the `hue` argument. For example, to recreate the plot from above without having to use a loop, we can utilize [`seaborn.scatterplot()`](https://seaborn.pydata.org/generated/seaborn.scatterplot.html) with `hue=\"region_name\"`. To scale the point sizes by population, we can specify `size=\"population\"` then use the `sizes` arguments to give a tuple that defines the smallest point size and the largest point size in arbitrary plot units. As before, you might need to play around with the tuple values in `sizes` until you find a combination that looks good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dff6a9-b474-4ace-846a-007667ec1a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "sns.scatterplot(\n",
    "    data=data2020,\n",
    "    x=\"gdp_per_capita\",\n",
    "    y=\"life_exp\",\n",
    "    hue=\"region_name\",\n",
    "    size=\"population\",\n",
    "    sizes=(10, 5000),\n",
    "    alpha=0.5,\n",
    "    legend=False,\n",
    "    ax=ax,\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"GDP per Capita\")\n",
    "plt.ylabel(\"Life Expectancy\")\n",
    "plt.title(\"2020\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6a7b9-211b-4584-b436-42c6f06cda92",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating Interactive Visualizations\n",
    "\n",
    "While the static scatter plot above is quite pretty to look at, it is not the most informative. We have no idea which points represent which countries and many countries appear clustered together, which makes it harder to tell them apart. An interactive visualization would allow for better exploration and investigation of the data. The easiest way of creating an interactive visualization out of a Pandas DataFrame is to use [HVPlot](https://hvplot.holoviz.org/), which is built on top of [Bokeh](https://bokeh.org/) and [HoloViews](https://holoviews.org/) and utilizes them in the background. Importing the `hvplot.pandas` module as we did before adds a new [`pandas.DataFrame.hvplot`](https://hvplot.holoviz.org/user_guide/Plotting.html#the-plot-interface) interface that allows for the creation of interactive plots using a syntax very similar to that of [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html).\n",
    "\n",
    "We can easily create an interactive version of the scatter plot from before by using [`pandas.DataFrame.hvplot.scatter()`](https://hvplot.holoviz.org/reference/pandas/scatter.html) with the following arguments:\n",
    "\n",
    "- `x` and `y` -- the column names for the data plotted on the X and Y axes respectively\n",
    "- `c` -- the column name that defines the groups or values based on which to color the points by\n",
    "- `s` -- the column name that defines values to use as point sizes\n",
    "- `scale` -- scaling factor to use when deriving point sizes from values specified by `s` (we will use $1 \\div max(X) \\times y$, where $X$ is the column specified in `s` and $y$ is an arbitrary scaling factor)\n",
    "- `hover_cols` -- fields to include in the tooltips in addition to those specified in `x`, `y`, `c`, and `s`\n",
    "- `alpha` -- transparency factor\n",
    "- `logx` -- whether to apply a logarithmic transformation on the X axis\n",
    "- `width` and `height` -- the size of the visualization in pixels\n",
    "\n",
    "Take some time to explore the the interactive visualization using the available controls. Experiment with panning and zooming and hover over various points to explore the tooltips.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7405796e-fef1-493f-8d5a-ec3b24b48e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2020.hvplot.scatter(\n",
    "    x=\"gdp_per_capita\",\n",
    "    y=\"life_exp\",\n",
    "    c=\"region_name\",\n",
    "    s=\"population\",\n",
    "    scale=(1 / data2020.population.max() * 2000000),\n",
    "    hover_cols=[\"country_name\", \"country_code\"],\n",
    "    alpha=0.5,\n",
    "    logx=True,\n",
    "    width=650,\n",
    "    height=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d435e3b9-627b-4c72-9b72-ff97c3d78d75",
   "metadata": {},
   "source": [
    "An alternative to HVPlot is [Plotly](https://plotly.com/python/), which is a popular interactive visualization library used in many programming languages. It consists of a complex ecosystem of various modules, but the [`plotly.express`](https://plotly.com/python/plotly-express/) module is the most popular and easiest to use. The syntax of [`plotly.express`](https://plotly.com/python/plotly-express/) is very similar to that of HVPlot. The biggest difference between the two libraries is that [`plotly.express`](https://plotly.com/python/plotly-express/) does not handle missing data and expects the input DataFrame to not contain any missing values. Hence we must drop all rows with missing values from the table using [`pandas.DataFrame.dropna()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) before passing it onto Plotly.\n",
    "\n",
    "We can create an interactive scatter plot via Plotly using the [`plotly.express.scatter()`](https://plotly.com/python-api-reference/generated/plotly.express.scatter) function along with the following arguments (note the similarities between HVPlot):\n",
    "\n",
    "- `data_frame` -- the [`pandas.DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) to use for the visualization with rows containing missing values removed\n",
    "- `x` and `y` -- the column names for the data plotted on the X and Y axes respectively\n",
    "- `color` -- the column name that defines the groups or values based on which to color the points by\n",
    "- `size` -- the column name that defines values to use as point sizes\n",
    "- `size_max` -- the size of the largest point in pixels (used to scale all point sizes)\n",
    "- `hover_name` -- the column name that defines the values to be used as tooltip titles\n",
    "- `hover_data` -- fields to include in the tooltip in addition to those specified in `x`, `y`, `color`, `size`, and `hover_name`\n",
    "- `opacity` -- transparency factor\n",
    "- `log_x` -- whether to apply a logarithmic transformation on the X axis\n",
    "- `width` and `height` -- the size of the visualization in pixels\n",
    "\n",
    "As before, make sure to explore the the interactive visualization using the available controls. Note how the tooltips and controls differ from those provided by HVPlot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d2513-9c64-4c80-ba8a-85b0d62d44c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "    data_frame=data2020.dropna(),\n",
    "    x=\"gdp_per_capita\",\n",
    "    y=\"life_exp\",\n",
    "    color=\"region_name\",\n",
    "    size=\"population\",\n",
    "    size_max=40,\n",
    "    hover_name=\"country_name\",\n",
    "    hover_data=[\"country_code\"],\n",
    "    opacity=0.5,\n",
    "    log_x=True,\n",
    "    width=650,\n",
    "    height=600,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37e0d1ba-1b6d-4f08-b233-ea5b7b21c729",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Working with Time Series\n",
    "\n",
    "Thus far we have covered the basics of working with data in Python, including reading CSV files, manipulating and reshaping data, joining tables, and creating both static and interactive visualizations. This covers the majority of the most essential data analysis workflows you might need. However, there are two major topics we have yet to discuss -- working with time series and aggregating data by group. We will explore these concepts using rapid transit ridership data from the Massachusetts Bay Transportation Authority (MBTA). Once we have covered these two final topics, you should have all the skills you need to begin your Python data analysis journey.\n",
    "\n",
    "The dataset we will use is a CSV file named [`mbta-gated-entries-2020.csv`](./data/mbta-gated-entries-2020.csv) located in the `data` directory. Each row in the table represents an unique 30-minute service time period for a specific MBTA rapid transit station and line combination in 2020. The columns are as follows:\n",
    "\n",
    "- `service_date` -- date in [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) `YYYY-MM-DD` format\n",
    "- `time period` -- timestamp denoting the start of the 30-minute time period in a somewhat unusual `(HH:mm:ss)` format\n",
    "- `stop_id` -- unique identifier for the rapid transit stop\n",
    "- `station_name` -- name of the rapid transit stop\n",
    "- `route_or_line` -- route or line served by the stop\n",
    "- `gated_entires` -- number of gated entries at the specified stop for the specified line or route in the specified time period\n",
    "\n",
    "Let us read this dataset into a [`pandas.DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) called `mbta` using [`pandas.read_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) and explore it via [`pandas.DataFrame.head()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) and [`pandas.DataFrame.dtypes`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da825e69-21a8-48db-9d35-8e1b44d4a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta = pd.read_csv(\"data/mbta-gated-entries-2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2f5e4-12a5-412a-af9b-2a25a8ba4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded3ff4e-6118-4765-87d6-7ed05fdb8109",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0f8209-24c8-4d42-a111-499d7fd81f78",
   "metadata": {},
   "source": [
    "Note how both the `service_date` and `time_period` have the datatype of `object`, indicating that they are stored as text. This does not allow us to treat these values as proper timestamps, limiting our options for quantitative analysis. To fix this, we should combine the `service_date` and `time_period` into a single timestamp using [`pandas.to_datetime()`](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html). But first we must clean the `time_period` values, which are all in parentheses for some reason.\n",
    "\n",
    "To strip the `time_period` values of the parentheses, we can utilize the [`pandas.Series.str`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.html) interface that allows us to apply string methods on the whole column. This allows us to apply a vectorized version of the built-in [`str.strip()`](https://docs.python.org/3/library/stdtypes.html#str.strip) method to the whole column via [`pandas.Series.str.strip()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.strip.html).\n",
    "\n",
    "Hence we can do the following all in one command:\n",
    "\n",
    "1. Extract the `time_period` column as a [`pandas.Series`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) object.\n",
    "2. Utilize [`pandas.Series.str.strip()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.strip.html) to remove the parentheses from the values, creating a new [`pandas.Series`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) object.\n",
    "3. Replace the `time_period` column with this new [`pandas.Series`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) object where the parentheses have been removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de97266-29eb-4636-8c39-2c2a9598d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta[\"time_period\"] = mbta.time_period.str.strip(\"()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd3ba7c-e7f7-4d43-b364-00123c17eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e4495e-c737-4efe-a762-586691d8ec0d",
   "metadata": {},
   "source": [
    "We can concatenate [`pandas.Series`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) objects containing textual data the same way we can concatenate strings in Python. Knowing this, we can easily combine the `service_date` and `time_period` columns into a single timestamp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c997f35a-3863-47d3-9818-c48ef2f9f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta[\"timestamp\"] = mbta.service_date + \" \" + mbta.time_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19879ce4-eb48-450f-80db-4081f9566b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d55c0-46cc-4486-aad4-01d9c2cc1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e138bf-6c07-4078-99ca-77c8c4dbaab2",
   "metadata": {},
   "source": [
    "Although the text in the new `timestamp` column sure looks like a valid timestamp, it is still just textual data and has no meaning to Python or Pandas. To covert these textual timestamps into Pandas-aware timestamps, we can use [`pandas.to_datetime()`](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html) and pass the `timestamp` series as input. This will produce a new series that we will use to replace the `timestamp` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689037a-1c17-4317-a337-29173cc64e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta[\"timestamp\"] = pd.to_datetime(mbta.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d30c8e-4d84-4e60-b420-a8a3b9fcc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c582dd-a0b3-4d89-a543-fb0559f8431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43029853-aca5-4037-ac89-20f269c49904",
   "metadata": {},
   "source": [
    "Note how `timestamp` now has a datatype of `datetime64` (with nanosecond precision). This allows us to perform arithmetic and comparisons on the timestamps and also utilize various additional date-time methods (like extracting the month or weekday for example) via the [`pandas.Series.dt`](https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.html) interface.\n",
    "\n",
    "Let us simplify our further analysis by dropping the redundant `service_date` and `time_period` columns using the [`pandas.DataFrame.drop()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a4df6-fc0c-42a3-a8a3-96f5cd9431ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.drop(columns=[\"service_date\", \"time_period\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f367cec-b28c-4612-b69e-6b9f64621278",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830a467-c4fa-425c-9496-e8a12afbc7b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Simple Aggregations\n",
    "\n",
    "We can easily get the total number of gated entries across the whole MBTA system in 2020 by using [`pandas.Series.sum()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.sum.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1ddf0-1ae6-4581-aa12-24846e6a8700",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.gated_entries.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e459c2aa-1c5f-42f7-ac12-64ddfd62ff2d",
   "metadata": {},
   "source": [
    "Combining [`pandas.Series.sum()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.sum.html) with boolean indexing allows us to extract the total number of gated entries for specific stations, lines, or even dates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf650b70-557e-4b3c-b8d5-678c4a0df04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.gated_entries[mbta.station_name == \"Davis\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a04b5-0155-4b8a-aba6-0becba33cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.gated_entries[mbta.route_or_line == \"Red Line\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb45560b-70d1-4203-98e7-5189c6bf246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.gated_entries[mbta.timestamp == \"2020-02-24\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca34fb-d914-44d8-b355-84f0bcf871de",
   "metadata": {},
   "source": [
    "Having the timestamps in `datetime64` format allows us to extract specific time periods using comparisons. For example, we can get the total number of gated entries across the whole MBTA system in February 2020 as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bfdbb7-de9a-4747-b81e-bd29a64ac356",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.gated_entries[\n",
    "    (mbta.timestamp >= \"2020-02-01\") & (mbta.timestamp < \"2020-03-01\")\n",
    "].sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0c62c27-8c59-437a-885b-5a9f620b85f0",
   "metadata": {},
   "source": [
    "Alternatively, we could take advantage of [`pandas.Series.dt.month`](https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.month.html) to extract the month numbers of the `datetype64` values and use that to get the same information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ce330-d90f-4c9f-ae4c-ab204ce4b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.gated_entries[mbta.timestamp.dt.month == 2].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d10ecc-8398-4a68-b6f1-e152e13c622b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Aggregating by Group\n",
    "\n",
    "Let us say we would like to get the number of gated entries across the whole MBTA system for each day in 2020. Pandas provides easy functionality to calculate various aggregate values by group, as long as there is a categorical column that defines the groups. Currently we only have a datetime column, which is not categorical and hence not suitable for aggregating entires by date. However, the `service_date` column we removed would have been perfect for this task. Luckily we can easily recreate this column using [`pandas.Series.dt.date`](https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.date.html) to extract the date from the `datetime64` timestamp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be27e06a-1982-4eb2-ba1a-d44e869f6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta[\"date\"] = mbta.timestamp.dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a043fde-fbfe-42d2-91f4-8e3de5a2b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edc8437-be54-41a9-b0a7-51fc69c85899",
   "metadata": {},
   "source": [
    "Now we can use [`pandas.DataFrame.groupby()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) to convert the [`pandas.DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) into a [`pandas.groupby.DataFrameGroupBy`](https://pandas.pydata.org/docs/reference/groupby.html) object, where all the values of the DataFrame are grouped by the specified categorical variable and any methods will apply by group. Note that this is no longer a DataFrame, so we cannot display it as such.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9cb365-dffa-4f5a-98f0-4b072206f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.groupby(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd21c1-77ed-41ab-9dab-fac489fe4f47",
   "metadata": {},
   "source": [
    "We can extract the desired column from this [`pandas.groupby.DataFrameGroupBy`](https://pandas.pydata.org/docs/reference/groupby.html) object as a [`pandas.groupby.SeriesGroupBy`](https://pandas.pydata.org/docs/reference/groupby.html) object, where any methods called on the series will apply by the previously defined groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af4ed8d-ed0e-4b3d-b6cc-03c60835362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.groupby(\"date\").gated_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29fa5d8-7a9d-4561-b282-3a20434e678f",
   "metadata": {},
   "source": [
    "When we call [`pandas.groupby.GroupBy.sum()`](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.sum.html) on this [`pandas.groupby.SeriesGroupBy`](https://pandas.pydata.org/docs/reference/groupby.html) object, we will get a new [`pandas.Series`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) object where all the gated entries for each unique date have been added together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b6772d-65ba-4612-a9b5-d339b519148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.groupby(\"date\").gated_entries.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ccc10-d00a-4804-869c-8297cc945d16",
   "metadata": {},
   "source": [
    "We can convert this [`pandas.Series`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) into a [`pandas.DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) using [`pandas.Series.to_frame()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.to_frame.html). We can also specify a new name for the column containing the aggregated values if desired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8eb482-690e-464e-b710-f0e8f20a3ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.groupby(\"date\").gated_entries.sum().to_frame(\"total_entries\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22b5337-25ae-4696-9b57-93b58041c637",
   "metadata": {},
   "source": [
    "Note how the groups make up the index of the new DataFrame. We can use [`pandas.DataFrame.reset_index()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html) to convert the dates back into a column and reset the index to a numerical one ranging from zero to one less than the number of rows. We can chain all the methods from before together and create a new DataFrame called `mbta_daily_sum` that contains the total number of gated entries across the MBTA system for each date in 2020.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155aa25-5e43-4b05-ab3a-0cf14711c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta_daily_sum = (\n",
    "    mbta.groupby(\"date\").gated_entries.sum().to_frame(\"total_entries\").reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569f3f24-7bb8-4927-86be-641ee7cb388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta_daily_sum.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c98344-cfe9-46ad-86f5-f797e8186950",
   "metadata": {},
   "source": [
    "If we wanted to find out which date had the most ridership, we could use [`pandas.Series.max()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.max.html) to get the maximum number of gated entries and then utilize boolean indexing to find out which date it corresponds to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ece1e3-1e99-49c8-bbcc-12b62df0ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta_daily_sum.total_entries.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a12ce-b116-4391-a131-98b3c5f7ef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta_daily_sum.date[mbta_daily_sum.total_entries == mbta_daily_sum.total_entries.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4ee24-7963-4fc2-b321-3e16c4db0385",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta_daily_sum.date[\n",
    "    mbta_daily_sum.total_entries == mbta_daily_sum.total_entries.max()\n",
    "].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa7421-acab-4080-8012-d3fe1dbda7af",
   "metadata": {},
   "source": [
    "Alternatively, we could use [`pandas.Series.argmax()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.argmax.html) to extract the index of the row with the most ridership and then utilize [`pandas.DataFrame.loc[]`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html) to extract said row using its index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb88e26-c1f2-4d39-864d-7771c2e60c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta_daily_sum.total_entries.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717d52c-d7d5-4fe9-9c56-e5876726756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta_daily_sum.loc[mbta_daily_sum.total_entries.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777588f-648c-44b2-bf2f-5ace2a10302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta_daily_sum.loc[mbta_daily_sum.total_entries.argmax(), \"date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a6c11e-9d4a-43a8-9d93-a559febcdf81",
   "metadata": {},
   "source": [
    "Finally, we could utilize [`pandas.DataFrame.sort_values()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html) to sort the values by total number of gated entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad8fe3c-86a6-416a-986d-03e85cb81c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta_daily_sum.sort_values(\"total_entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5b004f-ff3e-4325-b9ee-e6e06a159d40",
   "metadata": {},
   "source": [
    "Knowing all of this, we can easily take a quick look at the most and least used rapid transit stations and lines across the MBTA system in 2020 by combining the following:\n",
    "\n",
    "- [`pandas.DataFrame.groupby()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html)\n",
    "- [`pandas.groupby.GroupBy.sum()`](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.sum.html)\n",
    "- [`pandas.Series.sort_values()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.sort_values.html)\n",
    "- [`pandas.Series.to_frame()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.to_frame.html)\n",
    "- [`pandas.DataFrame.reset_index()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606621c4-0f55-4ec3-a972-505f8ba53bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    mbta.groupby(\"station_name\")\n",
    "    .gated_entries.sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .to_frame(\"total_entries\")\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d12bafe-a9a4-4e23-9b35-bc6dc88c7157",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    mbta.groupby(\"route_or_line\")\n",
    "    .gated_entries.sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .to_frame(\"total_entries\")\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f33d09-adcf-4fa5-85ae-4a5cf38efd8a",
   "metadata": {},
   "source": [
    "We can also group by multiple columns. For example, we can group by `date`, `station_name`, and `route_or_line` to create a new DataFrame `mbta_daily`, where the gated entries for each station and line combination are shown in 24-hour intervals instead of 30-minute intervals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315e0338-5411-4e83-8e21-32a7b949e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta_daily = (\n",
    "    mbta.groupby([\"date\", \"station_name\", \"route_or_line\"])\n",
    "    .gated_entries.sum()\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1d4c7-779e-4fe8-92e2-192c0d361dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5757853b-1848-4abd-9dfe-157655d60050",
   "metadata": {},
   "source": [
    "This new DataFrame will allow us to perform further analysis that does not require 30-minute temporal resolution and where a daily resolution is suitable. For example, we could look at the daily number of gated entries at the Harvard Square MBTA station throughout 2020 and see whether the onset of the global pandemic had an effect on ridership.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aac7baf-7141-4092-b803-2cc3d5981a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "mbta_daily[(mbta_daily.station_name == \"Harvard\")].plot(\n",
    "    x=\"date\", y=\"gated_entries\", legend=False, color=\"crimson\", ax=ax\n",
    ")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Gated Entries\")\n",
    "plt.title(\"2020 Daily Gated Entires at the Harvard Square MBTA Station\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09993721-00f0-49c2-b26d-32be219dfe66",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "Interactive [Kaggle tutorials](https://www.kaggle.com/learn) with built-in exercises:\n",
    "\n",
    "- Introduction to Python: https://www.kaggle.com/learn/python\n",
    "- Introduction to Pandas: https://www.kaggle.com/learn/pandas\n",
    "- Data Cleaning with Pandas: https://www.kaggle.com/learn/data-cleaning\n",
    "- Data Visualization using Seaborn: https://www.kaggle.com/learn/data-visualization\n",
    "\n",
    "Official [Pandas](https://pandas.pydata.org/pandas-docs/stable) resources:\n",
    "\n",
    "- Pandas Getting Started Guide: https://pandas.pydata.org/pandas-docs/stable/getting_started\n",
    "- Pandas User Guide: https://pandas.pydata.org/pandas-docs/stable/user_guide\n",
    "- Pandas API Reference: https://pandas.pydata.org/pandas-docs/stable/reference\n",
    "- Pandas Cheat Sheet: https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf\n",
    "\n",
    "Official [Matplotlib](https://matplotlib.org) resources:\n",
    "\n",
    "- Matplotlib Tutorials: https://matplotlib.org/stable/tutorials\n",
    "- Matplotlib User Guide: https://matplotlib.org/stable/users\n",
    "- Matplotlib Plot Types: https://matplotlib.org/stable/plot_types\n",
    "- Matplotlib Examples Gallery: https://matplotlib.org/stable/gallery\n",
    "- Matplotlib API Reference: https://matplotlib.org/stable/api\n",
    "- Matplotlib Cheat Sheets: https://matplotlib.org/cheatsheets\n",
    "\n",
    "Official [Seaborn](https://seaborn.pydata.org) resources:\n",
    "\n",
    "- Seaborn User Guide and Tutorial: https://seaborn.pydata.org/tutorial\n",
    "- Seaborn Examples Gallery: https://seaborn.pydata.org/examples\n",
    "- Seaborn API Reference: https://seaborn.pydata.org/api\n",
    "\n",
    "Official [HVPlot](https://hvplot.holoviz.org) resources:\n",
    "\n",
    "- HVPlot User Guide: https://hvplot.holoviz.org/user_guide\n",
    "- HVPlor Examples Gallery: https://hvplot.holoviz.org/reference\n",
    "\n",
    "Official [Plotly](https://plotly.com/python) resources:\n",
    "\n",
    "- Plotly Express User Guide: https://plotly.com/python/plotly-express\n",
    "- Plotly Python Graphing Library: https://plotly.com/python\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-data-viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
